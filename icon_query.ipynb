{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first block of code is just importing the requirements of the project\n",
    "\n",
    "import sys \n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from scipy import stats as sstats\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import shuffle\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "import collections\n",
    "import random\n",
    "import mahotas \n",
    "from sklearn.preprocessing import normalize\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import logging\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block of code is importing all the various parts of the project from their respective modules\n",
    "from icon_util import *\n",
    "from methods import *\n",
    "from aberrations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 28, 27, 16, 35, 78, 81, 64, 67, 26]\n",
      "[39, 45, 49, 86, 68, 84, 61, 95, 79, 56]\n",
      "[93, 24, 38, 31, 65, 22, 73, 37, 50, 29]\n",
      "[32, 51, 6, 87, 47, 94, 75, 36, 63, 52]\n",
      "[10, 46, 1, 74, 62, 71, 76, 43, 83, 23]\n",
      "[7, 58, 14, 3, 2, 15, 59, 80, 48, 82]\n",
      "[57, 77, 19, 12, 97, 60, 20, 41, 0, 53]\n",
      "[42, 13, 4, 92, 69, 91, 8, 17, 40, 55]\n",
      "[90, 33, 70, 21, 85, 88, 11, 34, 30, 98]\n",
      "[66, 18, 44, 72, 25, 54, 89, 99, 5, 96]\n"
     ]
    }
   ],
   "source": [
    "x=[i for i in range(100)]\n",
    "random.shuffle(x)\n",
    "for i in chunks(x):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_name = \"icon10\"\n",
    "\n",
    "# simple way to load the complete dataset (for a more sophisticated generator example, see LLD-logo script)\n",
    "# open hdf5 file\n",
    "hdf5_file = h5py.File('LLD-icon.hdf5', 'r')\n",
    "# load data into memory as numpy array\n",
    "#images, labels = (hdf5_file['data'][:], hdf5_file['labels/resnet/rc_64'][:])\n",
    "\n",
    "# alternatively, h5py objects can be used like numpy arrays without loading the whole dataset into memory:\n",
    "images, _ = (hdf5_file['data'], hdf5_file['labels/resnet/rc_64'])\n",
    "# here, images[0] will be again returned as a numpy array and can eg. be viewed with matplotlib using\n",
    "images = [np.transpose(i) if i.shape[0] == 3 else i for i in images[:10]]\n",
    "print(len(images))\n",
    "\n",
    "method_classes = [zernike_method, orb_method, neural_method, sift_method, contour_method]\n",
    "#method_classes = [orb_method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this if you want to generate the databases\n",
    "generate_databases(images, method_classes, image_set_name)\n",
    "methods = load_databases(method_classes, image_set_name)\n",
    "print(\"Loading Completed\")\n",
    "#####################\n",
    "# main loop \t#####\n",
    "#####################\n",
    "results, scores = run(methods, images, aberrations[:1])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the logs into a single dataframe for processing\n",
    "log_files = glob(\"Logs/*\") # these are the logs that we're loading\n",
    "print(log_files)\n",
    "joined_logs = pd.concat([pd.read_csv(i) for i in log_files])\n",
    "print(joined_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ranking\")\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "joined_logs[joined_logs['method']=='zernike_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[0,0])\n",
    "axes[0,0].set_title('Zernike')\n",
    "joined_logs[joined_logs['method']=='sift_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[0,1])\n",
    "axes[0,1].set_title('Sift')\n",
    "joined_logs[joined_logs['method']=='orb_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[1,0])\n",
    "axes[1,0].set_title('Orb')\n",
    "joined_logs[joined_logs['method']=='neural_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[1,1])\n",
    "axes[1,1].set_title('Neural')\n",
    "joined_logs[joined_logs['method']=='contour_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[2,0])\n",
    "axes[2,0].set_title('Contour')\n",
    "joined_logs[joined_logs['method']=='combined_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[2,1])\n",
    "axes[2,1].set_title('Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Timing\")\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2)\n",
    "joined_logs[joined_logs['method']=='zernike_method']['time'].hist(bins=20, figsize=(10,10), ax=axes[0,0])\n",
    "axes[0,0].set_title('Zernike')\n",
    "axes[0,0].set(xlim=(0,0.2))\n",
    "joined_logs[joined_logs['method']=='sift_method']['time'].hist(bins=20, figsize=(10,10), ax=axes[0,1])\n",
    "axes[0,1].set_title('Sift')\n",
    "#axes[0,1].set(xlim=(0,0.2))\n",
    "joined_logs[joined_logs['method']=='orb_method']['time'].hist(bins=20, figsize=(10,10), ax=axes[1,0])\n",
    "axes[1,0].set_title('Orb')\n",
    "axes[1,0].set(xlim=(0,0.2))\n",
    "joined_logs[joined_logs['method']=='neural_method']['time'].hist(bins=20, figsize=(10,10), ax=axes[1,1])\n",
    "axes[1,1].set_title('Neural')\n",
    "axes[1,1].set(xlim=(0,0.2))\n",
    "joined_logs[joined_logs['method']=='contour_method']['time'].hist(bins=20, figsize=(10,10), ax=axes[2,0])\n",
    "axes[2,0].set_title('Contour')\n",
    "#axes[2,0].set(xlim=(0,0.2))\n",
    "joined_logs[joined_logs['method']=='combined_method']['time'].hist(bins=20, figsize=(10,10), ax=axes[2,1])\n",
    "axes[2,1].set_title('Combined')\n",
    "axes[2,1].set(xlim=(0,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_logs[joined_logs['method']=='neural_method']['rank'].plot.hist(bins=100)\n",
    "# joined_logs.plot.kde()\n",
    "# pd.plotting.andrews_curves(joined_logs['rank'], 'method')\n",
    "# print(list('ABCD'))\n",
    "\n",
    "#joined_logs.groupby('method').plot.hist()\n",
    "#joined_logs[['method', 'rank']].hist(by='method')\n",
    "#joined_logs[['method','rank','aberration']].groupby('method').plot.hist(bins=100, label='method')\n",
    "#joined_logs.groupby('method').plot(kind='hist', subplots=True, figsize=(6,6))\n",
    "#joined_logs[joined_logs['method']=='zernike_method'].hist(bins=20, figsize=(20,20))\n",
    "#joined_logs[(joined_logs['method']=='zernike_method') & (joined_logs['aberration']=='ab_id')]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "joined_logs[joined_logs['method']=='combined_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[0])\n",
    "axes[0].set_title('Weighted Combined')\n",
    "joined_logs[joined_logs['method']=='uwcombined_method']['rank'].hist(bins=20, figsize=(10,10), ax=axes[1])\n",
    "axes[1].set_title('Unweighted Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## plot query image and top 11 hits, just for checking purposes  \n",
    "# plot_results(Data, matched_list_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=range(5)\n",
    "for i in a:\n",
    "    for j in a:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load scores from log files\n",
    "# success = []\n",
    "# contour_score = []\n",
    "# zernike_score = []\n",
    "# sift_score = []\n",
    "# orb_score = []\n",
    "# image = []\n",
    "# indices = []\n",
    "# mutation = []\n",
    "# contour_orig = []\n",
    "# sift_orig = []\n",
    "\n",
    "# # loop through image indices of existing queries \n",
    "# for fname in tqdm_notebook(os.listdir('.\\Logs')):\n",
    "# \tx1 = fname.find('_')+1\n",
    "# \tx2 = fname.find('.')\n",
    "# \ti = int(fname[x1:x2])\n",
    "# \tindices.append(i)\n",
    "\n",
    "# \tj = 0\n",
    "# \t#open log file of query\n",
    "# \twith open('C:/Users/kchad/Documents/LogoMatch/Logs/' + fname,\"r\") as myfile:\n",
    "# \t\tfor line in myfile:\n",
    "# \t\t\tj += 1\n",
    "# \t\t\tif j % 8 == 0:\n",
    "# \t\t\t\tfor k in range(0,result_len):\n",
    "# \t\t\t\t\tmutation.append(line[11:])\n",
    "# \t\t\tif j % 8 == 1:\n",
    "# \t\t\t\tresult_len = 0           \n",
    "# \t\t\t#  line has the results \n",
    "# \t\t\tif j % 8 == 7:\n",
    "# \t\t\t\tresult = eval(line[13:])\n",
    "# \t\t\t\tresult_len = len(result)\n",
    "# \t\t\t\tfirst_contour = result[0][1]\n",
    "# \t\t\t\tfirst_zernike = result[0][2]\n",
    "# \t\t\t\tfirst_sift = result[0][3]\n",
    "# \t\t\t\tfirst_orb = result[0][4]\n",
    "\t\t\t\t\n",
    "# \t\t\t\t# check if the given query result was the original image, 1 for correct, 0 for incorrect\n",
    "# \t\t\t\tfor elem in result:\n",
    "# \t\t\t\t\tif elem[0] == i:\n",
    "# \t\t\t\t\t\tsuccess.append(1)\n",
    "# \t\t\t\t\telse:\n",
    "# \t\t\t\t\t\tsuccess.append(0)\n",
    "\t\t\t\t\t\t\n",
    "# \t\t\t\t\t# normalize the contour/zernike scores\n",
    "# \t\t\t\t\tif first_contour > 0:                    \n",
    "# \t\t\t\t\t\tcontour_score.append( round(elem[1]/first_contour, 3) )\n",
    "# \t\t\t\t\telse:\n",
    "# \t\t\t\t\t\tcontour_score.append(0)    \n",
    "                        \n",
    "# \t\t\t\t\tif first_zernike > 0:                         \n",
    "# \t\t\t\t\t\tzernike_score.append( round(elem[2]/first_zernike, 3) )\n",
    "# \t\t\t\t\telse:\n",
    "# \t\t\t\t\t\tzernike_score.append(0)                        \n",
    "# \t\t\t\t\tif first_sift > 0:\n",
    "# \t\t\t\t\t\tsift_score.append( round(elem[3]/first_sift, 3) )\n",
    "# \t\t\t\t\telse:\n",
    "# \t\t\t\t\t\tsift_score.append(0)\n",
    "                        \n",
    "# \t\t\t\t\tif first_orb > 0:\n",
    "# \t\t\t\t\t\torb_score.append( round(elem[4]/first_orb, 3) )\n",
    "# \t\t\t\t\telse:\n",
    "# \t\t\t\t\t\torb_score.append(0)                        \n",
    "\n",
    "# \t\t\t\t\timage.append(i)\n",
    "# \t\t\t\t\tcontour_orig.append(elem[1])     \n",
    "# \t\t\t\t\tsift_orig.append(elem[3]) \n",
    "\n",
    "# ### save all the scores in a csv file\n",
    "\n",
    "# input_score = np.array(list(zip(contour_score,zernike_score,sift_score,orb_score,image,success,mutation,contour_orig,sift_orig)))\n",
    "# input_score = pd.DataFrame(input_score,columns = ['contour_score','zernike_score','sift_score','orb_score','image','success','mutation','contour_orig','sift_orig'])\n",
    "# print( input_score.head )\n",
    "# input_score.to_csv (r'input_scores.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### logsitic regression for weights of 4 methods\n",
    "# numbers = [i for i in range(100,200)]\n",
    "\n",
    "# ### randomly create training and test set\n",
    "# random.shuffle(numbers)\n",
    "# training = numbers[0:60]\n",
    "# testing = numbers[60:100]\n",
    "\n",
    "# ### search for a particular mutation\n",
    "# mutation_string = 'ab_line_circle'\n",
    "# print(mutation_string)\n",
    "\n",
    "# ### set threshold for contour and sift, set accuracy cutoff (top 10 or top 20)\n",
    "# contour_thresh = 0\n",
    "# sift_thresh = 0\n",
    "# rank_cutoff = 20\n",
    "\n",
    "# ### create training set, filter for mutation name and thresholds \n",
    "# input_score_mutation = input_score[(input_score[\"mutation\"].str.find(mutation_string) > -1) & (input_score[\"image\"].astype(float).isin(training)) & ((input_score[\"contour_orig\"].astype(float) >= contour_thresh) & (input_score[\"sift_orig\"].astype(float) >= sift_thresh))]\n",
    "# input_score_mutation = input_score_mutation.values\n",
    "\n",
    "# ### create test data set, not used for training\n",
    "# input_score_test = input_score[(input_score[\"mutation\"].str.find(mutation_string) > -1) & (input_score[\"image\"].astype(float).isin(testing) ) & ((input_score[\"contour_orig\"].astype(float) >= contour_thresh) & (input_score[\"sift_orig\"].astype(float) >= sift_thresh))]\n",
    "# input_score_test = pd.DataFrame(input_score_test,columns = ['contour_score','zernike_score','sift_score','orb_score','image','success','mutation','contour_orig','sift_orig'])\n",
    "\n",
    "# ### run logistic regression on training data\n",
    "# clf = LogisticRegression(random_state = 0, solver = 'lbfgs', multi_class = 'multinomial').fit(input_score_mutation[:,(0,1,2,3)], input_score_mutation[:,5])\n",
    "# logistic_weights = clf.coef_[0]/sum(clf.coef_[0])\n",
    "# print('Logistic Weights')\n",
    "# print(logistic_weights)\n",
    "\n",
    "# ### use weights to calculate weighted score and rank on training set\n",
    "# input_score_mutation = pd.DataFrame(input_score_mutation,columns = ['contour_score','zernike_score','sift_score','orb_score','image','success','mutation','contour_orig','sift_orig'])\n",
    "\n",
    "# input_score_mutation[\"contour_score\"] = input_score_mutation[\"contour_score\"].astype(float)\n",
    "# input_score_mutation[\"zernike_score\"] = input_score_mutation[\"zernike_score\"].astype(float)\n",
    "# input_score_mutation[\"sift_score\"] = input_score_mutation[\"sift_score\"].astype(float)\n",
    "# input_score_mutation[\"orb_score\"] = input_score_mutation[\"orb_score\"].astype(float)\n",
    "\n",
    "# input_score_mutation[\"avg_score\"] = .25*input_score_mutation[\"contour_score\"] + .25*input_score_mutation[\"zernike_score\"] + .25*input_score_mutation[\"sift_score\"] + .25*input_score_mutation[\"orb_score\"]\n",
    "# input_score_mutation[\"weight_score\"] = logistic_weights[0]*input_score_mutation[\"contour_score\"] + logistic_weights[1]*input_score_mutation[\"zernike_score\"] + logistic_weights[2]*input_score_mutation[\"sift_score\"] + logistic_weights[3]*input_score_mutation[\"orb_score\"]\n",
    "# input_score_mutation['Avg_Rank'] = input_score_mutation.groupby(['image','mutation'])['avg_score'].rank(ascending=False)\t\n",
    "# input_score_mutation['Weight_Rank'] = input_score_mutation.groupby(['image','mutation'])['weight_score'].rank(ascending=False)\t\n",
    "\n",
    "# ### use weights to calculate weighted score and rank on test set\n",
    "# input_score_test[\"contour_score\"] = input_score_test[\"contour_score\"].astype(float)\n",
    "# input_score_test[\"zernike_score\"] = input_score_test[\"zernike_score\"].astype(float)\n",
    "# input_score_test[\"sift_score\"] = input_score_test[\"sift_score\"].astype(float)\n",
    "# input_score_test[\"orb_score\"] = input_score_test[\"orb_score\"].astype(float)\n",
    "\n",
    "# input_score_test[\"avg_score\"] = .25*input_score_test[\"contour_score\"] + .25*input_score_test[\"zernike_score\"] + .25*input_score_test[\"sift_score\"] + .25*input_score_test[\"orb_score\"]\n",
    "# input_score_test[\"weight_score\"] = logistic_weights[0]*input_score_test[\"contour_score\"] + logistic_weights[1]*input_score_test[\"zernike_score\"] + logistic_weights[2]*input_score_test[\"sift_score\"] + logistic_weights[3]*input_score_test[\"orb_score\"]\n",
    "# input_score_test['Avg_Rank'] = input_score_test.groupby(['image','mutation'])['avg_score'].rank(ascending=False)\t\n",
    "# input_score_test['Weight_Rank'] = input_score_test.groupby(['image','mutation'])['weight_score'].rank(ascending=False)\t\n",
    "\n",
    "# ### check the rank of the original image, see if image was within the top 10 or top 20 search results\n",
    "# successes = input_score_mutation[input_score_mutation[\"success\"].astype(float) == 1]\n",
    "\n",
    "# avg_rank = sum(successes['Avg_Rank'])/successes.shape[0]\n",
    "# weight_rank = sum(successes['Weight_Rank'])/successes.shape[0]\n",
    "# top10_avg_rank = len( successes[(successes['Avg_Rank'] <= rank_cutoff)] )\n",
    "# top10_weight_rank = len( successes[(successes['Weight_Rank'] <= rank_cutoff)] )\n",
    "\n",
    "# print('Training Accuracy')\n",
    "# print((top10_avg_rank,top10_weight_rank))\n",
    "\n",
    "# successes_t = input_score_test[input_score_test[\"success\"].astype(float) == 1]\n",
    "\n",
    "# avg_rank_t = sum(successes_t['Avg_Rank'])/successes_t.shape[0]\n",
    "# weight_rank_t = sum(successes_t['Weight_Rank'])/successes_t.shape[0]\n",
    "# top10_avg_rank_t = len( successes_t[(successes_t['Avg_Rank'] <= rank_cutoff)] )\n",
    "# top10_weight_rank_t = len( successes_t[(successes_t['Weight_Rank'] <= rank_cutoff)] )\n",
    "\n",
    "# print('Testing Accuracy')\n",
    "# print((top10_avg_rank_t,top10_weight_rank_t))\n",
    "\n",
    "# #input_score.to_csv(path_or_buf='./score_result_2.csv', sep=',',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### display an image\n",
    "# image_index = 50\n",
    "# Data = gray(images[image_index])\n",
    "# Data, mutation = ab_random(Data, n = 9)\n",
    "\n",
    "# imgplot = plt.imshow(Data, cmap=plt.cm.gray)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testd = {}\n",
    "print(testd)\n",
    "print(testd.get(0, []))\n",
    "print(testd)\n",
    "print(testd.setdefault(0, []).append(\"heyy\"))\n",
    "print(testd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'b': ['x', 'y', 'y', 'x', 'y', 'x', 'x'], 'c': np.random.randn(7)})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2[df2['c'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
